{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2: Armado de un esquema de aprendizaje automático\n",
    "\n",
    "En el laboratorio final se espera que puedan poner en práctica los conocimientos adquiridos en el curso, trabajando con un conjunto de datos de clasificación.\n",
    "\n",
    "El objetivo es que se introduzcan en el desarrollo de un esquema para hacer tareas de aprendizaje automático: selección de un modelo, ajuste de hiperparámetros y evaluación.\n",
    "\n",
    "El conjunto de datos a utilizar está en `./data/loan_data.csv`. Si abren el archivo verán que al principio (las líneas que empiezan con `#`) describen el conjunto de datos y sus atributos (incluyendo el atributo de etiqueta o clase).\n",
    "\n",
    "Se espera que hagan uso de las herramientas vistas en el curso. Se espera que hagan uso especialmente de las herramientas brindadas por `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos y división en entrenamiento y evaluación\n",
    "\n",
    "La celda siguiente se encarga de la carga de datos (haciendo uso de pandas). Estos serán los que se trabajarán en el resto del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/loan_data.csv\", comment=\"#\")\n",
    "seed = 0\n",
    "\n",
    "# División entre instancias y etiquetas\n",
    "X, y = dataset.iloc[:, 1:], dataset.TARGET\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# división entre entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((1483, 10), (371, 10))"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TARGET</th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4700</td>\n      <td>88026.0</td>\n      <td>115506.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>182.248332</td>\n      <td>0.0</td>\n      <td>27.0</td>\n      <td>29.209023</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>19300</td>\n      <td>39926.0</td>\n      <td>101208.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>140.051638</td>\n      <td>0.0</td>\n      <td>14.0</td>\n      <td>31.545694</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>5700</td>\n      <td>71556.0</td>\n      <td>79538.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>92.643085</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>41.210012</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>13000</td>\n      <td>44875.0</td>\n      <td>57713.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>184.990324</td>\n      <td>1.0</td>\n      <td>12.0</td>\n      <td>28.602076</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>19300</td>\n      <td>72752.0</td>\n      <td>106084.0</td>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>193.707100</td>\n      <td>1.0</td>\n      <td>13.0</td>\n      <td>30.686106</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1849</th>\n      <td>1</td>\n      <td>53400</td>\n      <td>228236.0</td>\n      <td>305514.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>11.148069</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>34.558417</td>\n    </tr>\n    <tr>\n      <th>1850</th>\n      <td>1</td>\n      <td>53600</td>\n      <td>235895.0</td>\n      <td>299772.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>112.748282</td>\n      <td>7.0</td>\n      <td>22.0</td>\n      <td>44.945929</td>\n    </tr>\n    <tr>\n      <th>1851</th>\n      <td>1</td>\n      <td>53600</td>\n      <td>208197.0</td>\n      <td>297280.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>160.485251</td>\n      <td>2.0</td>\n      <td>29.0</td>\n      <td>41.646731</td>\n    </tr>\n    <tr>\n      <th>1852</th>\n      <td>1</td>\n      <td>65500</td>\n      <td>205156.0</td>\n      <td>290239.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>98.808206</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>144.189001</td>\n    </tr>\n    <tr>\n      <th>1853</th>\n      <td>1</td>\n      <td>77400</td>\n      <td>87651.0</td>\n      <td>224630.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>73.469630</td>\n      <td>3.0</td>\n      <td>13.0</td>\n      <td>40.929793</td>\n    </tr>\n  </tbody>\n</table>\n<p>1854 rows × 11 columns</p>\n</div>",
      "text/plain": "      TARGET   LOAN   MORTDUE     VALUE   YOJ  DEROG  DELINQ       CLAGE  \\\n0          0   4700   88026.0  115506.0   6.0    0.0     0.0  182.248332   \n1          0  19300   39926.0  101208.0   4.0    0.0     0.0  140.051638   \n2          0   5700   71556.0   79538.0   2.0    0.0     0.0   92.643085   \n3          0  13000   44875.0   57713.0   0.0    1.0     0.0  184.990324   \n4          0  19300   72752.0  106084.0  11.0    0.0     0.0  193.707100   \n...      ...    ...       ...       ...   ...    ...     ...         ...   \n1849       1  53400  228236.0  305514.0   6.0    0.0     0.0   11.148069   \n1850       1  53600  235895.0  299772.0   5.0    0.0     0.0  112.748282   \n1851       1  53600  208197.0  297280.0   4.0    1.0     1.0  160.485251   \n1852       1  65500  205156.0  290239.0   2.0    0.0     0.0   98.808206   \n1853       1  77400   87651.0  224630.0   9.0    0.0     2.0   73.469630   \n\n      NINQ  CLNO     DEBTINC  \n0      0.0  27.0   29.209023  \n1      0.0  14.0   31.545694  \n2      0.0  15.0   41.210012  \n3      1.0  12.0   28.602076  \n4      1.0  13.0   30.686106  \n...    ...   ...         ...  \n1849   0.0   2.0   34.558417  \n1850   7.0  22.0   44.945929  \n1851   2.0  29.0   41.646731  \n1852   1.0  21.0  144.189001  \n1853   3.0  13.0   40.929793  \n\n[1854 rows x 11 columns]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Documentación:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Descripción de los Datos y la Tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿De qué se trata el conjunto de datos?\n",
    "\n",
    "El *dataset* contiene un conjunto de datos que describen el comportamiento crediticio histórico de los clientes de un banco que hayan solicitado presetamos recientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Cuál es la variable objetivo que hay que predecir? ¿Qué significado tiene?\n",
    "\n",
    "La variable objetivo `TARGET` se trata de una variable binaria que puede asumir el valor 1, si el cliente no pagó el credito solicitado, y 0 caso contrario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Qué información (atributos) hay disponible para hacer la predicción?\n",
    "\n",
    "- LOAN    Monto de préstamo requerido.\n",
    "- MORTDUE Saldo de crédito hipotecario existente.\n",
    "- VALUE   Valor actual de la propiedad.\n",
    "- YOJ     Años en el trabajo actual.\n",
    "- DEROG   Número de informes despectivos.\n",
    "- DELINQ  Número de créditos en estado moroso.\n",
    "- CLAGE   Linea de crédito más antigua en meses.\n",
    "- NINQ    Número reciente de lineas de crédito.\n",
    "- CLNO    Número de lineas de crédito.\n",
    "- DEBTINC Cociente de deuda e ingresos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Qué atributos imagina ud. que son los más determinantes para la predicción?\n",
    "\n",
    ". LOAN\n",
    ". DELINQ\n",
    ". DEBTINC\n",
    ". DEROG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de la variable `TARGET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0.16925151719487525, 0.15633423180592992)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"TARGET\"].value_counts()\n",
    "nof_targets_train = len(y_train)\n",
    "nof_ones_train = np.sum(y_train == 1)\n",
    "nof_zeros_train = np.sum(y_train == 0)\n",
    "\n",
    "nof_targets_test = len(y_test)\n",
    "nof_ones_test = np.sum(y_test == 1)\n",
    "nof_zeros_test = np.sum(y_test == 0)\n",
    "\n",
    "(nof_ones_train / nof_targets_train, nof_ones_test / nof_targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Predicción con Modelos Lineales\n",
    "\n",
    "En este ejercicio se entrenarán modelos lineales de clasificación para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deberán utilizar la clase SGDClassifier de scikit-learn.\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/sgd.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1: SGDClassifier con hiperparámetros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador SGDClassifier usando los valores por omisión de scikit-learn para todos los parámetros. Únicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92       313\n",
      "           1       0.62      0.28      0.38        58\n",
      "\n",
      "    accuracy                           0.86       371\n",
      "   macro avg       0.75      0.62      0.65       371\n",
      "weighted avg       0.84      0.86      0.84       371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 (N): Pagó el prestamo.\n",
    "- 1 (P): No pagó el prestamo.\n",
    "- TP: Casos predichos que no pagarón y no pagaron efectivamente.\n",
    "- TF: Casos predichos que pagaron y pagaron efectivamente.\n",
    "- FP: Casos predichos que P (No pagó el prestamo) y pasó N (Pagó el prestamo).\n",
    "- FN: Casos predichos que N (Pagó el prestamo) y pasó P (No pagó el prestamo).\n",
    "\n",
    "- Recall: TP / (TP + FN)\n",
    "- Precision: TP / (TP + FP)\n",
    "- F1-score : 2*(Precision*Recall/Precision + Recall)\n",
    "\n",
    "Preferimos no tener FN a costa de algunos FP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(303, 10, 42, 16)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f087060e8b0>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 2 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 10)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2: Ajuste de Hiperparámetros\n",
    "\n",
    "Seleccionar valores para los hiperparámetros principales del SGDClassifier. Como mínimo, probar diferentes funciones de loss, tasas de entrenamiento y tasas de regularización.\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuración encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/benjamin/miniconda3/envs/diplodatos-introml/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: Check other params\n",
    "param_grid = {\n",
    "    'loss': ['hinge','log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "    'penalty': ['l2','l1']  \n",
    "}\n",
    "\n",
    "model = SGDClassifier()\n",
    "cv = GridSearchCV(model, param_grid, scoring=[\"recall\", \"accuracy\", \"precision\", \"f1\"], cv=5, refit=False)\n",
    "cv.fit(X, y)\n",
    "\n",
    "results = cv.cv_results_\n",
    "params = results['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n       'param_alpha', 'param_loss', 'param_penalty', 'params',\n       'split0_test_recall', 'split1_test_recall', 'split2_test_recall',\n       'split3_test_recall', 'split4_test_recall', 'mean_test_recall',\n       'std_test_recall', 'rank_test_recall', 'split0_test_accuracy',\n       'split1_test_accuracy', 'split2_test_accuracy', 'split3_test_accuracy',\n       'split4_test_accuracy', 'mean_test_accuracy', 'std_test_accuracy',\n       'rank_test_accuracy', 'split0_test_precision', 'split1_test_precision',\n       'split2_test_precision', 'split3_test_precision',\n       'split4_test_precision', 'mean_test_precision', 'std_test_precision',\n       'rank_test_precision', 'split0_test_f1', 'split1_test_f1',\n       'split2_test_f1', 'split3_test_f1', 'split4_test_f1', 'mean_test_f1',\n       'std_test_f1', 'rank_test_f1'],\n      dtype='object')"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>param_alpha</th>\n      <th>param_loss</th>\n      <th>param_penalty</th>\n      <th>mean_test_recall</th>\n      <th>std_test_recall</th>\n      <th>mean_test_accuracy</th>\n      <th>std_test_accuracy</th>\n      <th>mean_test_precision</th>\n      <th>std_test_precision</th>\n      <th>rank_test_recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>48</th>\n      <td>0.1</td>\n      <td>perceptron</td>\n      <td>l2</td>\n      <td>0.507403</td>\n      <td>0.111129</td>\n      <td>0.802056</td>\n      <td>0.015790</td>\n      <td>0.422108</td>\n      <td>0.029756</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.0001</td>\n      <td>perceptron</td>\n      <td>l2</td>\n      <td>0.442887</td>\n      <td>0.168812</td>\n      <td>0.805299</td>\n      <td>0.067199</td>\n      <td>0.473997</td>\n      <td>0.170153</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.01</td>\n      <td>perceptron</td>\n      <td>l2</td>\n      <td>0.436277</td>\n      <td>0.143566</td>\n      <td>0.810145</td>\n      <td>0.022808</td>\n      <td>0.434016</td>\n      <td>0.046238</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.0001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.429984</td>\n      <td>0.167337</td>\n      <td>0.848445</td>\n      <td>0.039927</td>\n      <td>0.605196</td>\n      <td>0.130070</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.0001</td>\n      <td>modified_huber</td>\n      <td>l1</td>\n      <td>0.429773</td>\n      <td>0.102380</td>\n      <td>0.806929</td>\n      <td>0.082851</td>\n      <td>0.529026</td>\n      <td>0.207343</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.0001</td>\n      <td>squared_hinge</td>\n      <td>l2</td>\n      <td>0.416605</td>\n      <td>0.163828</td>\n      <td>0.774510</td>\n      <td>0.046956</td>\n      <td>0.365671</td>\n      <td>0.135460</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.397515</td>\n      <td>0.145408</td>\n      <td>0.811755</td>\n      <td>0.044645</td>\n      <td>0.441848</td>\n      <td>0.129296</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.01</td>\n      <td>perceptron</td>\n      <td>l1</td>\n      <td>0.385563</td>\n      <td>0.171549</td>\n      <td>0.774452</td>\n      <td>0.089879</td>\n      <td>0.479405</td>\n      <td>0.210958</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00001</td>\n      <td>modified_huber</td>\n      <td>l2</td>\n      <td>0.378265</td>\n      <td>0.108766</td>\n      <td>0.798241</td>\n      <td>0.046413</td>\n      <td>0.420193</td>\n      <td>0.118232</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.001</td>\n      <td>perceptron</td>\n      <td>l2</td>\n      <td>0.378160</td>\n      <td>0.088615</td>\n      <td>0.817704</td>\n      <td>0.035154</td>\n      <td>0.466157</td>\n      <td>0.104415</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.0001</td>\n      <td>squared_hinge</td>\n      <td>l1</td>\n      <td>0.378054</td>\n      <td>0.172021</td>\n      <td>0.738418</td>\n      <td>0.045317</td>\n      <td>0.284013</td>\n      <td>0.087120</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.1</td>\n      <td>squared_hinge</td>\n      <td>l1</td>\n      <td>0.355050</td>\n      <td>0.173061</td>\n      <td>0.783167</td>\n      <td>0.021644</td>\n      <td>0.323878</td>\n      <td>0.110455</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.00001</td>\n      <td>perceptron</td>\n      <td>l1</td>\n      <td>0.342570</td>\n      <td>0.105817</td>\n      <td>0.794513</td>\n      <td>0.016136</td>\n      <td>0.375130</td>\n      <td>0.037787</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.001</td>\n      <td>squared_hinge</td>\n      <td>l2</td>\n      <td>0.339556</td>\n      <td>0.100837</td>\n      <td>0.850595</td>\n      <td>0.019910</td>\n      <td>0.584613</td>\n      <td>0.096095</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.336542</td>\n      <td>0.160378</td>\n      <td>0.832272</td>\n      <td>0.022277</td>\n      <td>0.489809</td>\n      <td>0.129581</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.00001</td>\n      <td>perceptron</td>\n      <td>l2</td>\n      <td>0.336489</td>\n      <td>0.194321</td>\n      <td>0.731348</td>\n      <td>0.078230</td>\n      <td>0.270120</td>\n      <td>0.168508</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.0001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.336330</td>\n      <td>0.094692</td>\n      <td>0.868927</td>\n      <td>0.017140</td>\n      <td>0.727538</td>\n      <td>0.070940</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.001</td>\n      <td>modified_huber</td>\n      <td>l1</td>\n      <td>0.333210</td>\n      <td>0.111684</td>\n      <td>0.868395</td>\n      <td>0.011331</td>\n      <td>0.738439</td>\n      <td>0.057756</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.00001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.332787</td>\n      <td>0.175465</td>\n      <td>0.783106</td>\n      <td>0.069199</td>\n      <td>0.382686</td>\n      <td>0.183567</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.0001</td>\n      <td>perceptron</td>\n      <td>l1</td>\n      <td>0.329825</td>\n      <td>0.132728</td>\n      <td>0.814973</td>\n      <td>0.027480</td>\n      <td>0.475402</td>\n      <td>0.143982</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.00001</td>\n      <td>squared_hinge</td>\n      <td>l2</td>\n      <td>0.329561</td>\n      <td>0.127779</td>\n      <td>0.829539</td>\n      <td>0.023990</td>\n      <td>0.507737</td>\n      <td>0.138205</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.001</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.323427</td>\n      <td>0.100456</td>\n      <td>0.868391</td>\n      <td>0.017681</td>\n      <td>0.735253</td>\n      <td>0.072037</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.310629</td>\n      <td>0.082293</td>\n      <td>0.866775</td>\n      <td>0.013375</td>\n      <td>0.735855</td>\n      <td>0.053821</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.001</td>\n      <td>squared_hinge</td>\n      <td>l1</td>\n      <td>0.310312</td>\n      <td>0.088929</td>\n      <td>0.856519</td>\n      <td>0.017565</td>\n      <td>0.638096</td>\n      <td>0.096797</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.307298</td>\n      <td>0.076500</td>\n      <td>0.868930</td>\n      <td>0.015036</td>\n      <td>0.759094</td>\n      <td>0.075290</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.297779</td>\n      <td>0.157974</td>\n      <td>0.822567</td>\n      <td>0.039185</td>\n      <td>0.529954</td>\n      <td>0.135536</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00001</td>\n      <td>modified_huber</td>\n      <td>l1</td>\n      <td>0.297462</td>\n      <td>0.141152</td>\n      <td>0.813897</td>\n      <td>0.034598</td>\n      <td>0.469110</td>\n      <td>0.128642</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.01</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.284717</td>\n      <td>0.075795</td>\n      <td>0.867853</td>\n      <td>0.013095</td>\n      <td>0.776858</td>\n      <td>0.052474</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00001</td>\n      <td>squared_hinge</td>\n      <td>l1</td>\n      <td>0.284188</td>\n      <td>0.121328</td>\n      <td>0.781036</td>\n      <td>0.036828</td>\n      <td>0.338220</td>\n      <td>0.065328</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.01</td>\n      <td>modified_huber</td>\n      <td>l2</td>\n      <td>0.281544</td>\n      <td>0.110113</td>\n      <td>0.866236</td>\n      <td>0.014815</td>\n      <td>0.778699</td>\n      <td>0.072239</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.001</td>\n      <td>modified_huber</td>\n      <td>l2</td>\n      <td>0.275198</td>\n      <td>0.078793</td>\n      <td>0.840903</td>\n      <td>0.036894</td>\n      <td>0.576749</td>\n      <td>0.224087</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.01</td>\n      <td>squared_hinge</td>\n      <td>l1</td>\n      <td>0.264833</td>\n      <td>0.126828</td>\n      <td>0.847311</td>\n      <td>0.046345</td>\n      <td>0.633306</td>\n      <td>0.252716</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.0001</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.261925</td>\n      <td>0.080703</td>\n      <td>0.841419</td>\n      <td>0.019730</td>\n      <td>0.617025</td>\n      <td>0.157947</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.01</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.258858</td>\n      <td>0.062699</td>\n      <td>0.872167</td>\n      <td>0.009596</td>\n      <td>0.910340</td>\n      <td>0.030895</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.01</td>\n      <td>modified_huber</td>\n      <td>l1</td>\n      <td>0.258805</td>\n      <td>0.092689</td>\n      <td>0.863539</td>\n      <td>0.013372</td>\n      <td>0.764048</td>\n      <td>0.050308</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.01</td>\n      <td>squared_hinge</td>\n      <td>l2</td>\n      <td>0.258593</td>\n      <td>0.103038</td>\n      <td>0.853801</td>\n      <td>0.030892</td>\n      <td>0.676797</td>\n      <td>0.194637</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.0001</td>\n      <td>modified_huber</td>\n      <td>l2</td>\n      <td>0.255420</td>\n      <td>0.101265</td>\n      <td>0.809572</td>\n      <td>0.038436</td>\n      <td>0.416408</td>\n      <td>0.141366</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.01</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.252353</td>\n      <td>0.069376</td>\n      <td>0.866233</td>\n      <td>0.012504</td>\n      <td>0.810894</td>\n      <td>0.057244</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.001</td>\n      <td>perceptron</td>\n      <td>l1</td>\n      <td>0.249075</td>\n      <td>0.116356</td>\n      <td>0.804719</td>\n      <td>0.038430</td>\n      <td>0.412081</td>\n      <td>0.149417</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.001</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.246007</td>\n      <td>0.036263</td>\n      <td>0.866237</td>\n      <td>0.009703</td>\n      <td>0.832277</td>\n      <td>0.081414</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.1</td>\n      <td>modified_huber</td>\n      <td>l2</td>\n      <td>0.239450</td>\n      <td>0.062248</td>\n      <td>0.863538</td>\n      <td>0.011520</td>\n      <td>0.795455</td>\n      <td>0.065071</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.0001</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.210206</td>\n      <td>0.101763</td>\n      <td>0.846795</td>\n      <td>0.026209</td>\n      <td>0.643949</td>\n      <td>0.181970</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.1</td>\n      <td>squared_hinge</td>\n      <td>l2</td>\n      <td>0.209942</td>\n      <td>0.083090</td>\n      <td>0.844107</td>\n      <td>0.019293</td>\n      <td>0.585167</td>\n      <td>0.184139</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.1</td>\n      <td>perceptron</td>\n      <td>l1</td>\n      <td>0.200000</td>\n      <td>0.400000</td>\n      <td>0.699280</td>\n      <td>0.267208</td>\n      <td>0.032973</td>\n      <td>0.065946</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.01</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.197567</td>\n      <td>0.052127</td>\n      <td>0.861386</td>\n      <td>0.007463</td>\n      <td>0.874955</td>\n      <td>0.028991</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.1</td>\n      <td>modified_huber</td>\n      <td>l1</td>\n      <td>0.165151</td>\n      <td>0.049734</td>\n      <td>0.858147</td>\n      <td>0.007310</td>\n      <td>0.921429</td>\n      <td>0.046754</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.1</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.145796</td>\n      <td>0.049562</td>\n      <td>0.855451</td>\n      <td>0.008220</td>\n      <td>0.924206</td>\n      <td>0.062698</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.1</td>\n      <td>log</td>\n      <td>l2</td>\n      <td>0.142464</td>\n      <td>0.048503</td>\n      <td>0.854370</td>\n      <td>0.007598</td>\n      <td>0.902540</td>\n      <td>0.059133</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.1</td>\n      <td>hinge</td>\n      <td>l1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.833334</td>\n      <td>0.000900</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.1</td>\n      <td>log</td>\n      <td>l1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.833334</td>\n      <td>0.000900</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>49</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   param_alpha      param_loss param_penalty  mean_test_recall  \\\n48         0.1      perceptron            l2          0.507403   \n18      0.0001      perceptron            l2          0.442887   \n38        0.01      perceptron            l2          0.436277   \n10      0.0001           hinge            l2          0.429984   \n15      0.0001  modified_huber            l1          0.429773   \n16      0.0001   squared_hinge            l2          0.416605   \n2      0.00001             log            l2          0.397515   \n39        0.01      perceptron            l1          0.385563   \n4      0.00001  modified_huber            l2          0.378265   \n28       0.001      perceptron            l2          0.378160   \n17      0.0001   squared_hinge            l1          0.378054   \n47         0.1   squared_hinge            l1          0.355050   \n9      0.00001      perceptron            l1          0.342570   \n26       0.001   squared_hinge            l2          0.339556   \n3      0.00001             log            l1          0.336542   \n8      0.00001      perceptron            l2          0.336489   \n12      0.0001             log            l2          0.336330   \n25       0.001  modified_huber            l1          0.333210   \n0      0.00001           hinge            l2          0.332787   \n19      0.0001      perceptron            l1          0.329825   \n6      0.00001   squared_hinge            l2          0.329561   \n22       0.001             log            l2          0.323427   \n21       0.001           hinge            l1          0.310629   \n27       0.001   squared_hinge            l1          0.310312   \n23       0.001             log            l1          0.307298   \n1      0.00001           hinge            l1          0.297779   \n5      0.00001  modified_huber            l1          0.297462   \n32        0.01             log            l2          0.284717   \n7      0.00001   squared_hinge            l1          0.284188   \n34        0.01  modified_huber            l2          0.281544   \n24       0.001  modified_huber            l2          0.275198   \n37        0.01   squared_hinge            l1          0.264833   \n13      0.0001             log            l1          0.261925   \n30        0.01           hinge            l2          0.258858   \n35        0.01  modified_huber            l1          0.258805   \n36        0.01   squared_hinge            l2          0.258593   \n14      0.0001  modified_huber            l2          0.255420   \n33        0.01             log            l1          0.252353   \n29       0.001      perceptron            l1          0.249075   \n20       0.001           hinge            l2          0.246007   \n44         0.1  modified_huber            l2          0.239450   \n11      0.0001           hinge            l1          0.210206   \n46         0.1   squared_hinge            l2          0.209942   \n49         0.1      perceptron            l1          0.200000   \n31        0.01           hinge            l1          0.197567   \n45         0.1  modified_huber            l1          0.165151   \n40         0.1           hinge            l2          0.145796   \n42         0.1             log            l2          0.142464   \n41         0.1           hinge            l1          0.000000   \n43         0.1             log            l1          0.000000   \n\n    std_test_recall  mean_test_accuracy  std_test_accuracy  \\\n48         0.111129            0.802056           0.015790   \n18         0.168812            0.805299           0.067199   \n38         0.143566            0.810145           0.022808   \n10         0.167337            0.848445           0.039927   \n15         0.102380            0.806929           0.082851   \n16         0.163828            0.774510           0.046956   \n2          0.145408            0.811755           0.044645   \n39         0.171549            0.774452           0.089879   \n4          0.108766            0.798241           0.046413   \n28         0.088615            0.817704           0.035154   \n17         0.172021            0.738418           0.045317   \n47         0.173061            0.783167           0.021644   \n9          0.105817            0.794513           0.016136   \n26         0.100837            0.850595           0.019910   \n3          0.160378            0.832272           0.022277   \n8          0.194321            0.731348           0.078230   \n12         0.094692            0.868927           0.017140   \n25         0.111684            0.868395           0.011331   \n0          0.175465            0.783106           0.069199   \n19         0.132728            0.814973           0.027480   \n6          0.127779            0.829539           0.023990   \n22         0.100456            0.868391           0.017681   \n21         0.082293            0.866775           0.013375   \n27         0.088929            0.856519           0.017565   \n23         0.076500            0.868930           0.015036   \n1          0.157974            0.822567           0.039185   \n5          0.141152            0.813897           0.034598   \n32         0.075795            0.867853           0.013095   \n7          0.121328            0.781036           0.036828   \n34         0.110113            0.866236           0.014815   \n24         0.078793            0.840903           0.036894   \n37         0.126828            0.847311           0.046345   \n13         0.080703            0.841419           0.019730   \n30         0.062699            0.872167           0.009596   \n35         0.092689            0.863539           0.013372   \n36         0.103038            0.853801           0.030892   \n14         0.101265            0.809572           0.038436   \n33         0.069376            0.866233           0.012504   \n29         0.116356            0.804719           0.038430   \n20         0.036263            0.866237           0.009703   \n44         0.062248            0.863538           0.011520   \n11         0.101763            0.846795           0.026209   \n46         0.083090            0.844107           0.019293   \n49         0.400000            0.699280           0.267208   \n31         0.052127            0.861386           0.007463   \n45         0.049734            0.858147           0.007310   \n40         0.049562            0.855451           0.008220   \n42         0.048503            0.854370           0.007598   \n41         0.000000            0.833334           0.000900   \n43         0.000000            0.833334           0.000900   \n\n    mean_test_precision  std_test_precision  rank_test_recall  \n48             0.422108            0.029756                 1  \n18             0.473997            0.170153                 2  \n38             0.434016            0.046238                 3  \n10             0.605196            0.130070                 4  \n15             0.529026            0.207343                 5  \n16             0.365671            0.135460                 6  \n2              0.441848            0.129296                 7  \n39             0.479405            0.210958                 8  \n4              0.420193            0.118232                 9  \n28             0.466157            0.104415                10  \n17             0.284013            0.087120                11  \n47             0.323878            0.110455                12  \n9              0.375130            0.037787                13  \n26             0.584613            0.096095                14  \n3              0.489809            0.129581                15  \n8              0.270120            0.168508                16  \n12             0.727538            0.070940                17  \n25             0.738439            0.057756                18  \n0              0.382686            0.183567                19  \n19             0.475402            0.143982                20  \n6              0.507737            0.138205                21  \n22             0.735253            0.072037                22  \n21             0.735855            0.053821                23  \n27             0.638096            0.096797                24  \n23             0.759094            0.075290                25  \n1              0.529954            0.135536                26  \n5              0.469110            0.128642                27  \n32             0.776858            0.052474                28  \n7              0.338220            0.065328                29  \n34             0.778699            0.072239                30  \n24             0.576749            0.224087                31  \n37             0.633306            0.252716                32  \n13             0.617025            0.157947                33  \n30             0.910340            0.030895                34  \n35             0.764048            0.050308                35  \n36             0.676797            0.194637                36  \n14             0.416408            0.141366                37  \n33             0.810894            0.057244                38  \n29             0.412081            0.149417                39  \n20             0.832277            0.081414                40  \n44             0.795455            0.065071                41  \n11             0.643949            0.181970                42  \n46             0.585167            0.184139                43  \n49             0.032973            0.065946                44  \n31             0.874955            0.028991                45  \n45             0.921429            0.046754                46  \n40             0.924206            0.062698                47  \n42             0.902540            0.059133                48  \n41             0.000000            0.000000                49  \n43             0.000000            0.000000                49  "
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[[\"param_alpha\", \"param_loss\", \"param_penalty\",\n",
    "            \"mean_test_recall\", \"std_test_recall\",\n",
    "            \"mean_test_accuracy\", \"std_test_accuracy\",\n",
    "            \"mean_test_precision\", \"std_test_precision\",\n",
    "            \"rank_test_recall\"]].sort_values(by=\"rank_test_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Árboles de Decisión\n",
    "\n",
    "En este ejercicio se entrenarán árboles de decisión para predecir la variable objetivo.\n",
    "\n",
    "Para ello, deberán utilizar la clase DecisionTreeClassifier de scikit-learn.\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/tree.html\n",
    "  - https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1: DecisionTreeClassifier con hiperparámetros por defecto\n",
    "\n",
    "Entrenar y evaluar el clasificador DecisionTreeClassifier usando los valores por omisión de scikit-learn para todos los parámetros. Únicamente **fijar la semilla aleatoria** para hacer repetible el experimento.\n",
    "\n",
    "Evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.2: Ajuste de Hiperparámetros\n",
    "\n",
    "Seleccionar valores para los hiperparámetros principales del DecisionTreeClassifier. Como mínimo, probar diferentes criterios de partición (criterion), profundidad máxima del árbol (max_depth), y cantidad mínima de samples por hoja (min_samples_leaf).\n",
    "\n",
    "Para ello, usar grid-search y 5-fold cross-validation sobre el conjunto de entrenamiento para explorar muchas combinaciones posibles de valores.\n",
    "\n",
    "Reportar accuracy promedio y varianza para todas las configuraciones.\n",
    "\n",
    "Para la mejor configuración encontrada, evaluar sobre el conjunto de **entrenamiento** y sobre el conjunto de **evaluación**, reportando:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- matriz de confusión\n",
    "\n",
    "\n",
    "Documentación:\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('diplodatos-introml': conda)",
   "name": "python396jvsc74a57bd0a3365ab711809cbd3c28073f2b04c8fbc851cda023e71afcb1b65ecfa437f8bb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}